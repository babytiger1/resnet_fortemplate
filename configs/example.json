{
  "exp_name": "example",
  "num_epochs": 300,
  "num_iter_per_epoch": 10,
  "learning_rate": 0.001,
  "batch_size": 512,
  "state_size": [784],
  "max_to_keep":5,
  "activation_fn": "relu",
  "dropout": 0.2,
  "batch_norm": 1,
  "hidden_layers": [256,128,128,64,64,32,16,16,8]
}


